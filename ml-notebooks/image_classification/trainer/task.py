# Copyright 2021 Verily Life Sciences LLC
#
# Use of this source code is governed by a BSD-style
# license that can be found in the LICENSE file or at
# https://developers.google.com/open-source/licenses/bsd


import argparse
import json
import logging
import os
from datetime import datetime
from pathlib import Path

import hypertune
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import trainer.utils as utils

PATCH_CAMELYON_LABELS = ["non_metastatic", "metastatic"]
FINE_TUNING_EPOCHS = 3


def _get_compiled_model(lr, image_height, image_width, num_classes):
    base_model = keras.applications.Xception(
        weights="imagenet",
        input_shape=(image_height, image_width, 3),
        include_top=False,
    )

    base_model.trainable = False

    inputs = keras.Input(shape=(image_height, image_width, 3))

    x = layers.Rescaling(1.0 / 255)(inputs)
    x = base_model(x, training=False)
    x = keras.layers.GlobalAveragePooling2D()(x)
    # x = keras.layers.Dropout(0.2)(x)
    outputs = keras.layers.Dense(num_classes, activation="softmax")(x)

    model = keras.Model(inputs, outputs)
    loss = tf.keras.losses.SparseCategoricalCrossentropy()

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr),
        loss=loss,
        metrics=["accuracy"],
    )
    return model


def main():

    logging.getLogger().setLevel(logging.INFO)
    parser = argparse.ArgumentParser(description="ML Trainer")
    parser.add_argument("--epochs", type=int, default=4)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--image-height", type=int, default=100)
    parser.add_argument("--image-width", type=int, default=336)

    parser.add_argument("--gcs-workdir", required=True)
    parser.add_argument("--gcs-model-savedir", required=True)
    parser.add_argument("--gcs-metrics-path", required=True)
    parser.add_argument("--ml-task", default="patchcamelyon")
    parser.add_argument("--fine-tune", default="true")

    parser.add_argument("--multi-node", default=False, action="store_true")
    parser.add_argument("--single-node", dest="multi-node", action="store_false")

    parser.add_argument("--hptune", default=False, action="store_true")
    parser.add_argument("--non-hptune", dest="hptune", action="store_false")

    args = parser.parse_args()
    logging.info("Tensorflow version %s", tf.__version__)

    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    print(f"timestamp: {timestamp}")

    log_dir = os.environ["AIP_TENSORBOARD_LOG_DIR"]
    print(f" using (autogenerated) tb log dir: {log_dir}")

    checkpoint_dir = f"{args.gcs_workdir}/checkpoints/{timestamp}/checkpoints"
    if args.hptune:  # add the trial id to the dir path
        trial_id = os.environ.get("CLOUD_ML_TRIAL_ID")
        checkpoint_dir = f"{checkpoint_dir}/{trial_id}"
    print(f"checkpoint dir: {checkpoint_dir}")

    if args.hptune:
        model_path_gcs = f"{args.gcs_model_savedir}/{trial_id}"
    else:
        if args.gcs_model_savedir == "AIP_MODEL_DIR" and os.getenv("AIP_MODEL_DIR"):
            model_path_gcs = os.getenv("AIP_MODEL_DIR")
        else:
            model_path_gcs = args.gcs_model_savedir
    print(f"GCS saved model path: {model_path_gcs}")

    (model_path_gcs, checkpoint_dir) = utils.get_model_dirs(
        model_path_gcs, checkpoint_dir
    )

    # define and compile the model
    print("creating the model..")
    if args.multi_node:
        print("using MultiWorkerMirroredStrategy")
        strategy = tf.distribute.MultiWorkerMirroredStrategy()
    else:
        strategy = tf.distribute.MirroredStrategy()

    print("Number of devices: {}".format(strategy.num_replicas_in_sync))

    image_size = (int(args.image_height), int(args.image_width))
    print(f"using image size: {image_size}")

    if args.ml_task == "patchcamelyon":
        LABELS = PATCH_CAMELYON_LABELS
        print("generating patch camelyon datasets")
        (training_set, validation_set, _, _) = utils.generate_camelyon_datasets(
            args.batch_size * strategy.num_replicas_in_sync
        )
    else:
        print(f"unknown task: {args.ml_task}")
        exit(1)

    if strategy.num_replicas_in_sync > 1:
        print("Using mirrored strategy.")
        with strategy.scope():
            model = _get_compiled_model(
                args.lr, int(args.image_height), int(args.image_width), len(LABELS)
            )
    else:
        model = _get_compiled_model(
            args.lr, int(args.image_height), int(args.image_width), len(LABELS)
        )

    model.summary()

    # define callbacks
    (tensorboard_callback, model_checkpoint_callback) = utils.define_callbacks(
        log_dir, checkpoint_dir
    )

    # train the model
    print(f"training the model with lr {args.lr}")
    model.fit(
        training_set,
        epochs=args.epochs,
        callbacks=[tensorboard_callback, model_checkpoint_callback],
        validation_data=validation_set,
    )

    # fine-tuning for patchcamelyon
    if args.ml_task == "patchcamelyon" and args.fine_tune == "true":
        print("Now fine-tuning the PatchCamelyon model")
        with strategy.scope():
            fine_tuning_epochs = FINE_TUNING_EPOCHS
            for layer in model.layers:
                layer.trainable = True
            model.compile(
                optimizer=keras.optimizers.Adam(learning_rate=1e-5),
                loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                metrics=["accuracy"],
            )

        model.fit(
            training_set,
            epochs=fine_tuning_epochs,
            callbacks=[tensorboard_callback, model_checkpoint_callback],
            validation_data=validation_set,
        )

    print("saving the model to GCS")
    try:
        model.save(model_path_gcs)
    except Exception as e:
        print(e)
        import time

        time.sleep(10)
        try:
            # retry once on error: Save the model to GCS
            print("retry saving the model to GCS")
            model.save(model_path_gcs)
        except Exception as e1:
            print(e1)

    # get some metrics info
    print(f"model history: {model.history.history}")
    try:
        val_accuracy = (model.history.history["val_accuracy"])[-1]
        val_loss = (model.history.history["val_loss"])[-1]
    except Exception as e:
        print(e)
        val_accuracy = 0
        val_loss = 0

    try:
        (ma, mp, mr, all_preds, all_labels) = utils.generate_metrics(
            validation_set, model, len(LABELS)
        )

        # write out metrics info for 'eval' component
        metrics_info = {
            "val_accuracy": val_accuracy,
            "val_loss": val_loss,
            "auc": f"{ma.result().numpy()}",
            "precision": f"{ma.result().numpy()}",
            "recall": f"{mr.result().numpy()}",
            "all_labels": f"{all_labels}",
            "all_preds": f"{all_preds}",
            "num_classes": len(LABELS),
        }
        print(f"AUC: {ma.result().numpy()}")
        print(f"Precision: {mp.result().numpy()}")
        print(f"Recall: {mr.result().numpy()}")

        metrics_info_str = json.dumps(metrics_info)
        print(f"metrics info json string: {metrics_info_str}")

        Path(args.gcs_metrics_path).mkdir(parents=True, exist_ok=True)
        metrics_info_file = f"{args.gcs_metrics_path}/metrics.json"

        print(f"writing metrics: {metrics_info_str} to {metrics_info_file}")
        with open(metrics_info_file, "w") as f:
            f.write(metrics_info_str)
    except Exception as e2:
        print(e2)

    hpt = hypertune.HyperTune()
    hpt.report_hyperparameter_tuning_metric(
        hyperparameter_metric_tag="accuracy",
        metric_value=val_accuracy,
        global_step=args.epochs,
    )


if __name__ == "__main__":
    main()
